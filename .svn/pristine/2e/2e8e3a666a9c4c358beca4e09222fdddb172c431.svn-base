package com.cn.zbt.crawlmeta.controller;

import java.io.IOException;
import java.sql.Timestamp;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.Map;
import org.apache.log4j.Logger;
import org.jsoup.Connection;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
import com.cn.zbt.crawlmeta.dm.CommonUtils;
import com.cn.zbt.crawlmeta.dm.Ctext;
import com.cn.zbt.crawlmeta.dm.GetService;
import com.cn.zbt.crawlmeta.dm.ReadFile;
import com.cn.zbt.crawlmeta.dm.SetProxy;
import com.cn.zbt.crawlmeta.service.ResultTabSer;

public class TianYa {
	private static final Logger logger = Logger.getLogger(TianYa.class);
	private static ResultTabSer resultTabService = (ResultTabSer) GetService
			.getInstance().getService("resultTabService");

	/**
	 * 爬取天涯网
	 * 
	 * @param url
	 * @return
	 */
	@SuppressWarnings("finally")
	public  Document fetch(String url,String q,String pn) {
		Document doc = null;
		Connection conn = null;
		try {
			//new SetIp().setIp();
			conn = Jsoup
					.connect(url)
					.header("User-Agent",
							"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36")
					.timeout(4000);
			conn.data("q", q).data("pn", pn);
			doc = conn.get();
		} catch (IOException e) {
			logger.error("jsoup访问天涯网异常==========>", e);
		} finally {
			return doc;
		}
	}
	/**
	 * 爬取天涯网
	 * 
	 * @param url
	 * @return
	 */
	@SuppressWarnings("finally")
	public  Document fetch(String url) {
		Document doc = null;
		Connection conn = null;
		try {
			//new SetIp().setIp();
			conn = Jsoup
					.connect(url)
					.header("User-Agent",
							"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36")
					.timeout(4000);
			doc = conn.get();
		} catch (IOException e) {
			doc = fetch_old(url);
			logger.error("jsoup访问天涯网异常==========>", e);
		} finally {
			return doc;
		}
	}

	@SuppressWarnings("finally")
	public Document fetch_old(String url ) {
		Document doc = null;
		Connection conn = null;
		try {
			new SetProxy().setIp();
			conn = Jsoup
					.connect(url)
					.header("User-Agent",
							"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36")
					.timeout(4000);
			doc = conn.get();
		} catch (IOException e) {
			logger.error("jsoup访问天涯网异常==========>", e);
		} finally {
			return doc;
		}
	 }

	public  void getDoc(String keyword) {
		int  i = 0;
		String reg="";
		do {
			reg="";
			String url = "http://search.tianya.cn/bbs";
			i=i+1;
			if(i>75)
			{
				logger.info("页数异常" + url);
				return;
			}
			try {
				logger.error("开始解析关键字为:“" + keyword +"” 页数为："+i );
				Document doc=new TianYa().fetch(url,keyword,i+"");
				if(doc.toString().contains("没有找到含有"))
				{
					logger.info("页数异常" + url);
					return;
				}
				reg=doc.getElementsByClass("long-pages").last().toString();
				getDate(doc, keyword);
				Thread.sleep(2000);
			} catch (Exception e) {
				e.printStackTrace();
				logger.error("解析错误，关键字为:" + keyword +"页数为："+i+ "。异常详情：" + e );
			}
			System.gc();
		} while (!reg.contains("<span>下一页</span>"));

	 }
	@SuppressWarnings({ "static-access" })
	public  void getDate(Document doc, String keyword) {
		Date sinatime_now = new Date();
		Elements elements = doc.select(".searchListOne>ul>li");
		
		for (int p = 0; p < elements.size() - 1; p++) {
			Element element = elements.get(p).select("div>h3>a").first();
			String title = "";
			String content = "";
			String url = "";
			url = element.attr("href");
			logger.info("正在处理：" + url);
			try {
				Document doc1 = new TianYa().fetch(url);
				title = doc1.select("title").first().text();
				String ctStr = new CommonUtils().getRegex("((\\d{2}|((1|2)\\d{3}))(-|年)\\d{2}(-|月)\\d{2}(日|)( \\d{1,2}:\\d{1,2}(:\\d{1,2}|)|))",
						 doc1.toString().replace("\n", "")
							.replace("\r", "").replace("&nbsp;", " ")).trim();
				Date pubdate = new Date();
				// 转换各种格式的日期
				pubdate = (new CommonUtils().matchDateString(ctStr) == null ? sinatime_now
						: new CommonUtils().matchDateString(ctStr));
				Date inittime = new Date();
				inittime = new CommonUtils()
						.matchDateString("1970-01-01 08:00:01");
				pubdate = pubdate.compareTo(inittime) < 0 ? inittime : pubdate;
				Ctext ctx = new Ctext();
				content = ctx.deleteLabel(doc1.toString());
				Map<Integer, String> map = ctx.splitBlock(content);
				// 数据库字段超长、后续可修改
				if (ctx.judgeBlocks(map).length() > 9999) {
					content = ctx.judgeBlocks(map).substring(0, 9999);
				} else {
					content = ctx.judgeBlocks(map);
				}
				// 作者
				String author = new CommonUtils().getRegex("楼主：(.*?)时间",
						ctx.deleteLabel(doc1.toString())).trim();
				author = (author.length() == 0 || author == null) ? "天涯网"
						: author;
				// 转发量，评论量。新闻稿有的不存在，需要后续处理，进行热度排序推送
				String zfs =new CommonUtils().getRegex("点击：(.*?)回复",
						ctx.deleteLabel(doc1.toString())).trim();
				zfs = (zfs.length() == 0 || zfs == null) ? "0" : zfs;

				String pls = new CommonUtils().getRegex("回复：(.*?)(<|[\u4e00-\u9fa5])", doc1.toString().replace("\n", "")
						.replace("\r", "").replace("&nbsp;", " ")).trim();
				pls = (pls.length() == 0 || pls == null) ? "0" : pls;

				resultTabService.insertRes(new CommonUtils().setMD5(url),title, url, content,
						Integer.valueOf(pls), Integer.valueOf(zfs),
						new Timestamp(pubdate.getTime()), keyword+"_天涯网", author,
						new Timestamp(sinatime_now.getTime()));
				logger.info("URL:" + url + "     " + "提取完成。");
			} catch (Exception e) {
				logger.error("解析错误:" + url + "错误详情： " + e);
			}
			 
		}
	}
 
	public void runInter() {
		List<String> keywords = new ArrayList<String>();
		keywords = new ReadFile().readKeyword();
		for (String keyword : keywords) {
			getDoc(keyword.trim());
			logger.info("----关键词:" + keyword + " 爬取结束----"
					+ new Date(System.currentTimeMillis()));
		}
	}

	public static void main(String[] args) throws IOException {
		logger.info("----爬取开始----" + new Date(System.currentTimeMillis()));

		TianYa sw = new TianYa();
		sw.runInter();
		 
		logger.info("----全部主页爬取结束----" + new Date(System.currentTimeMillis()));
	}
}
